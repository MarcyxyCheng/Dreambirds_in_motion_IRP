@ARTICLE{Beg2021,
  author={Beg, Marijan and Taka, Juliette and Kluyver, Thomas and Konovalov, Alexander and Ragan-Kelley, Min and Thi√©ry, Nicolas M. and Fangohr, Hans},
  journal={Computing in Science and Engineering}, 
  title={Using {J}upyter for Reproducible Scientific Workflows}, 
  year={2021},
  volume={23},
  number={2},
  pages={36-46},
  doi={10.1109/MCSE.2021.3052101},
  url = {https://doi.org/10.1109/MCSE.2021.3052101}
}

@ARTICLE{Gu2023,
  author={Gu, Shuai and others},
  title={AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023},
  url = {https://arxiv.org/abs/2307.04725}
}

@ARTICLE{Zhang2023,
  author={Zhang, Yunzhu and Agrawala, Maneesh},
  title={Adding Conditional Control to Text-to-Image Diffusion Models},
  journal={arXiv preprint arXiv:2302.05543},
  year={2023},
  URL = {https://arxiv.org/abs/2302.05543}
}

@ARTICLE{Ge2023,
  author={Ge, Yujun and Zhang, Yixiao and Du, Rui and others},
  title={TokenFlow: Consistent Diffusion Features for Consistent Video Editing},
  journal={arXiv preprint arXiv:2307.10373},
  year={2023},
  url = {https://arxiv.org/abs/2307.10373}
}

@ARTICLE{Unterthiner2018,
  author={Unterthiner, Thomas and Nessler, Bernhard and Heigold, Georg and others},
  title={Towards Accurate Generative Models of Video: A New Metric \& Challenges},
  journal={arXiv preprint arXiv:1812.01717},
  year= {2019},
  url = {https://arxiv.org/abs/1812.01717}
}

@ARTICLE{Ma2021,
  author={Ma, Chao and Liu, Yuming and Wang, Yong and others},
  title={Perceptual Video Quality Prediction: A Large-Scale Database and a CNN Model},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={10628--10637},
  url = {https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_Perceptual_Video_Quality_Prediction_A_Large-Scale_Database_and_a_CNN_CVPR_2021_paper.pdf}
}

@ARTICLE{Karras2019,
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  title={A Style-Based Generator Architecture for Generative Adversarial Networks},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={4401--4410},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf}
}

@ARTICLE{Huang2022,
  author    = {Huang, Zhewei and Zhang, Tianyuan and Heng, Pheng-Ann},
  title     = {Real-Time Intermediate Flow Estimation for Video Frame Interpolation (RIFE)},
  journal   = {arXiv preprint arXiv:2011.06294},
  year      = {2022},
  url = {https://arxiv.org/abs/2011.06294}
}

@ARTICLE{Hu2021,
  author    = {Hu, Edward J. and Shen, Yelong and Wallis, Phil and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Weizhu and Chen, Zichao},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal   = {arXiv preprint arXiv:2106.09685},
  year      = {2021},
  url = {https://arxiv.org/abs/2106.09685}
}

@ARTICLE{Brock2018,
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  title={Large Scale GAN Training for High Fidelity Natural Image Synthesis},
  journal={International Conference on Learning Representations (ICLR)},
  year={2019},
  url = {https://arxiv.org/abs/1809.11096}
}

@ARTICLE{Ruiz2023,
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and others},
  title={DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation},
  journal={arXiv preprint arXiv:2208.12242},
  year={2023},
  url = {https://arxiv.org/abs/2208.12242}
}

@ARTICLE{BarTal2022,
  author={Bar-Tal, Itai and Alaluf, Yuval and Shapira, Or and others},
  title={Text2LIVE: Text-Driven Layered Image and Video Editing},
  journal={European Conference on Computer Vision (ECCV)},
  year={2022},
  url = {https://arxiv.org/abs/2204.02491}
}

@ARTICLE{Wu2022,
  author={Wu, Jing and Ge, Yujun and Zhang, Yixiao and others},
  title={Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation},
  journal={arXiv preprint arXiv:2212.11565},
  year={2022},
  url = {https://arxiv.org/abs/2212.11565}
}

@ARTICLE{Yang2022,
  author={Yang, Zhen and Chen, Jiahui and He, Zexu and others},
  title={Diffusion Models for Video Generation},
  journal={NeurIPS},
  year={2022},
  url = {https://arxiv.org/abs/2209.04747}
}

@ARTICLE{Tian2023,
  author={Tian, Yuxin and Yu, Zongwei and Qian, Chen and others},
  title={VideoComposer: Compositional Video Synthesis with Motion Controllability},
  journal={arXiv preprint arXiv:2311.11591},
  year={2023},
  url = {https://arxiv.org/abs/2311.11591}
}

@ARTICLE{Xu2023,
  author={Xu, Da and Zhang, Yixiao and Duan, Jiahong and others},
  title={PYoCo: Pyramid-coordinated Progressive Noise Conditioning for Consistent Video Synthesis},
  journal={arXiv preprint arXiv:2308.14440},
  year={2023},
  url = {https://arxiv.org/abs/2308.14440}
}

@ARTICLE{Ge2023pyoco,
  author       = {Songwei Ge and Seungjun Nah and Guilin Liu and Tyler Poon and Andrew Tao and Bryan Catanzaro and David Jacobs and Jia-Bin Huang and Ming-Yu Liu and Yogesh Balaji},
  title        = {Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models},
  journal      = {arXiv preprint arXiv:2305.10474},
  year         = {2023},
  url          = {https://arxiv.org/abs/2305.10474},
  note         = {Version 3, last revised 26 Mar 2024}
}

@article{xing2023dynamicrafter,
  title={DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors},
  author={Xing, Jinbo and Xia, Menghan and Zhang, Yong and Chen, Haoxin and Yu, Wangbo and Liu, Hanyuan and Wang, Xintao and Wong, Tien-Tsin and Shan, Ying},
  journal={arXiv preprint arXiv:2310.12190},
  year={2023},
  url = {https://arxiv.org/abs/2310.12190}
}

@ARTICLE{Radford2021,
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and others},
  title={Learning Transferable Visual Models from Natural Language Supervision},
  journal={International Conference on Machine Learning (ICML)},
  year={2021},
  url = {https://arxiv.org/abs/2103.00020}
}

@ARTICLE{Patashnik2021,
  author={Patashnik, Or and Wu, Zongze and Shechtman, Eli and others},
  title={StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery},
  journal={IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  url = {https://arxiv.org/abs/2103.17249}
}

@ARTICLE{Wang2020,
  author={Wang, Yuwei and Qi, Xin and Zhang, Jianzhong and Wang, Zhaowen},
  title={Improved Video Generation for Multi-functional Applications},
  journal={arXiv preprint arXiv:2006.10738},
  year={2020},
  url = {https://arxiv.org/abs/2006.10738}
}

@ARTICLE{Radford2021a,
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Jack and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  title={Learning Transferable Visual Models From Natural Language Supervision},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021},
  url = {https://arxiv.org/abs/2103.00020}
}

@inproceedings{wah2011cub,
  title={The Caltech-UCSD Birds-200-2011 Dataset},
  author={Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  booktitle={Technical Report CNS-TR-2011-001},
  year={2011},
  institution={California Institute of Technology},
  url = {https://www.vision.caltech.edu/datasets/cub_200_2011/}
}

@inproceedings{cheng2023videobird,
  title={VideoBirds: A Dataset for Fine-Grained Bird Classification in Videos},
  author={Cheng, Zhiyuan and Li, Wenhao and Lu, Yang and Huang, Gao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  url = {UNVERIFIED}
}

@inproceedings{li2024fbdsurveillance,
  title={FBD-SV: A Large-Scale Flying Bird Detection Surveillance Video Dataset},
  author={Li, Xin and Zhou, Ke and Tang, Min and Chen, Haoliang},
  booktitle={Proceedings of the IEEE International Conference on Image Processing (ICIP)},
  year={2024},
  url = {https://arxiv.org/abs/2409.00317}
}

@ARTICLE{Sun2025,
  author={Sun, Zi-Wei and Hua, Ze-Xi and Li, Heng-Chao and Qi, Zhi-Peng and Li, Xiang and Li, Yan and Zhang, Jin-Chi},
  title={FBD-SV-2024: Flying Bird Object Detection Dataset in Surveillance Video},
  journal={Scientific Data},
  volume={12},
  number={1},
  pages={530},
  year={2025},
  doi={10.1038/s41597-025-04872-6},
  url={https://doi.org/10.1038/s41597-025-04872-6}
}

@ARTICLE{Ge2016,
  author={Ge, ZongYuan and McCool, Chris and Sanderson, Conrad and Wang, Peng and Liu, Lingqiao and Reid, Ian and Corke, Peter},
  title={Exploiting Temporal Information for DCNN-based Fine-Grained Object Classification},
  journal={International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
  year={2016},
  doi={10.1109/DICTA.2016.7797039},
  url={https://arxiv.org/abs/1608.00486}
}

@ARTICLE{Wang2023VideoComposer,
  author       = {Xiang Wang and Hangjie Yuan and Shiwei Zhang and Dayou Chen and Jiuniu Wang and Yingya Zhang and Yujun Shen and Deli Zhao and Jingren Zhou},
  title        = {VideoComposer: Compositional Video Synthesis with Motion Controllability},
  journal      = {arXiv preprint arXiv:2306.02018},
  year         = {2023},
  url          = {https://arxiv.org/abs/2306.02018},
  note         = {Version 2, last revised 6 Jun 2023}
}

@ARTICLE{Geyer2023TokenFlow,
  author       = {Michal Geyer and Omer Bar-Tal and Shai Bagon and Tali Dekel},
  title        = {TokenFlow: Consistent Diffusion Features for Consistent Video Editing},
  journal      = {arXiv preprint arXiv:2307.10373},
  year         = {2023},
  url          = {https://arxiv.org/abs/2307.10373},
  note         = {Version 3, last revised 20 Nov 2023}
}

@techreport{Wah2011CUB200,
  title={The Caltech-UCSD Birds-200-2011 Dataset},
  author={Wah, Catherine and Branson, Steve and Welinder, Pietro and Perona, Pietro and Belongie, Serge},
  institution={Caltech},
  number={CNS-TR-2011-001},
  year={2011},
  url = {https://www.vision.caltech.edu/datasets/cub_200_2011/}
}

@article{Yu2021AP10K,
  title={AP-10K: A Benchmark for Animal Pose Estimation in the Wild},
  author={Yu, Tianyu and Xu, Yufei and Peng, Xingyi and Wu, Yizhou and Chen, Lin},
  journal={arXiv preprint arXiv:2108.12617},
  year={2021},
  url = {https://arxiv.org/abs/2108.12617}
}

@inproceedings{Cao2019AnimalPose,
  title={Cross-domain adaptation for animal pose estimation},
  author={Cao, Jinkun and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  booktitle={ICCV},
  year={2019},
  url = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Cross-Domain_Adaptation_for_Animal_Pose_Estimation_ICCV_2019_paper.pdf}
}

@article{Mathis2018DeepLabCut,
  title={DeepLabCut: markerless pose estimation of user-defined body parts with deep learning},
  author={Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie W. and Bethge, Matthias},
  journal={Nature Neuroscience},
  volume={21},
  number={9},
  pages={1281--1289},
  year={2018},
  url = {https://www.nature.com/articles/s41593-018-0209-y}
}

@article{Pereira2020SLEAP,
  title={SLEAP: A deep learning system for multi-animal pose tracking},
  author={Pereira, Talmo D and others},
  journal={Nature Methods},
  volume={17},
  pages={1--9},
  year={2020},
  url = {https://www.nature.com/articles/s41592-022-01426-1}
}

@inproceedings{Naik2023POP,
  title={3D-POP: An automated annotation approach to facilitate markerless 2D-3D tracking of freely moving birds with marker-based motion capture},
  author={Naik, Hemal and Chan, Alex H. H. and Yang, Junran and Delacoux, Mathilde and Couzin, Iain D. and Kano, Fumihiro and Nagy, M{\'a}t{\'e}},
  booktitle={CVPR},
  year={2023},
  url = {https://openaccess.thecvf.com/content/CVPR2023/papers/Naik_3D-POP_-_An_Automated_Annotation_Approach_to_Facilitate_Markerless_2D-3D_CVPR_2023_paper.pdf}
}

@article{Waldmann2024MuPPET,
  title={3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking},
  author={Waldmann, Urs and Chan, Alex H. H. and Naik, Hemal and Nagy, M{\'a}t{\'e} and Couzin, Iain D. and Deussen, Oliver and Goldluecke, Bastian and Kano, Fumihiro},
  journal={International Journal of Computer Vision},
  year={2024},
  url = {https://link.springer.com/article/10.1007/s11263-024-02074-y}
}

@inproceedings{Cao2017PAF,
  title={Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  author={Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  booktitle={CVPR},
  year={2017},
  url = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.pdf}
}

@inproceedings{Rombach2022LDM,
  title={High-Resolution Image Synthesis with Latent Diffusion Models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  year={2022},
  url = {https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf}
}

@inproceedings{Zhang2023ControlNet,
  title={Adding Conditional Control to Text-to-Image Diffusion Models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle={ICCV},
  year={2023},
  url = {https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf}
}

@inproceedings{Ruiz2023DreamBooth,
  title={DreamBooth: Fine-Tuning Text-to-Image Diffusion Models for Subject-Driven Generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={CVPR},
  year={2023},
  url = {https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf}
}

@article{Gal2022TextualInversion,
  title={An image is worth one word: Personalizing text-to-image generation using textual inversion},
  author={Gal, Rinon and others},
  journal={arXiv preprint arXiv:2208.01618},
  year={2022},
  url = {https://arxiv.org/abs/2208.01618}
}

@article{Ye2023IPAdapter,
  title={IP-Adapter: Text-Compatible Image Prompt Adapter for Text-to-Image Diffusion Models},
  author={Ye, Hongchen and others},
  journal={arXiv preprint arXiv:2308.06721},
  year={2023},
  url = {https://arxiv.org/abs/2308.06721}
}

@article{Li2023BLIP2,
  title={BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and others},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023},
  url = {https://arxiv.org/abs/2301.12597}
}

@article{Liu2023LLaVA,
  title={Visual instruction tuning},
  author={Liu, Haotian and others},
  journal={NeurIPS},
  year={2023},
  url = {https://arxiv.org/abs/2304.08485}
}

@article{Alayrac2022Flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and others},
  journal={arXiv preprint arXiv:2204.14198},
  year={2022},
  url = {https://arxiv.org/abs/2204.14198}
}

@article{Blattmann2023SVD,
  title={Stable Video Diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023},
  url = {https://arxiv.org/abs/2311.15127}
}

@article{Guo2023AnimateDiff,
  title={AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and others},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023},
  url = {https://arxiv.org/abs/2307.04725}
}

@inproceedings{Wang2023VideoComposer,
  title={VideoComposer: Compositional Video Synthesis with Motion Controllability},
  author={Wang, Xing and others},
  booktitle={NeurIPS},
  year={2023},
  url = {https://arxiv.org/abs/2306.02018}
}

@inproceedings{Xing2024DynamiCrafter,
  title={DynamiCrafter: Animating open-domain images with video diffusion priors},
  author={Xing, Jun and others},
  booktitle={ECCV},
  year={2024},
  url = {https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06298.pdf}
}

@article{Tevet2022MDM,
  title={Human Motion Diffusion Model},
  author={Tevet, Guy and Raab, Sigal and Lipman, Yaron and Bermano, Amit H. and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2209.14916},
  year={2022},
  url = {https://arxiv.org/abs/2209.14916}
}

@article{Tevet2022MotionCLIP,
  title={MotionCLIP: Exposing Human Motion Generation to CLIP Space},
  author={Tevet, Guy and others},
  journal={arXiv preprint arXiv:2203.08063},
  year={2022},
  url = {https://arxiv.org/abs/2203.08063}
}


@inproceedings{Yan2023TECO,
  title={Temporally Consistent Transformers for Video Generation},
  author={Yan, Wenqi and others},
  booktitle={ICML},
  year={2023},
  url = {https://arxiv.org/abs/2210.02396}
}

@inproceedings{Karras2023DreamPose,
  title={DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion},
  author={Karras, Tero and others},
  booktitle={ICCV},
  year={2023},
  url = {https://openaccess.thecvf.com/content/ICCV2023/papers/Karras_DreamPose_Fashion_Image-to-Video_Synthesis_via_Stable_Diffusion_ICCV_2023_paper.pdf}
}

@misc{MusePose2024,
  title={MusePose: A Pose-Driven Image-to-Video Framework for Virtual Human},
  author={MusePose Team},
  year={2024},
  url = {https://github.com/TMElyralab/MusePose}
}

@inproceedings{Andriluka2014MPII,
  title={2D Human Pose Estimation: New Benchmark and State of the Art Analysis},
  author={Andriluka, Mykhaylo and others},
  booktitle={CVPR},
  year={2014},
  url = {https://openaccess.thecvf.com/content_cvpr_2014/papers/Andriluka_2D_Human_Pose_2014_CVPR_paper.pdf}
}

@article{Ionescu2014H36M,
  title={Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},
  author={Ionescu, Catalin and others},
  journal={TPAMI},
  year={2014},
  url = {https://vision.imar.ro/human3.6m/pami-h36m.pdf}
}

@inproceedings{Heusel2017FID,
  title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
  author={Heusel, Martin and others},
  booktitle={NeurIPS},
  year={2017},
  url = {https://papers.nips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf}
}

@inproceedings{Zhang2018LPIPS,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Zhang, Richard and others},
  booktitle={CVPR},
  year={2018},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.pdf}
}

@inproceedings{Huang2024VBench,
  title={VBench: Comprehensive Benchmark Suite for Video Generative Models},
  author={Huang, Zhen and others},
  booktitle={CVPR},
  year={2024},
  url = {https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_VBench_Comprehensive_Benchmark_Suite_for_Video_Generative_Models_CVPR_2024_paper.pdf}
}

@inproceedings{Ge2016DICTA,
  author    = {Ge, ZongYuan and McCool, Chris and Sanderson, Conrad and Wang, Peng and Liu, Lingqiao and Reid, Ian and Corke, Peter},
  title     = {Exploiting Temporal Information for DCNN-Based Fine-Grained Object Classification},
  booktitle = {Proceedings of the International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
  year      = {2016},
  pages     = {1--6},
  publisher = {IEEE},
  doi       = {10.1109/DICTA.2016.7797039},
  url       = {https://arxiv.org/abs/1608.00486}
}

@dataset{VB100Zenodo,
  author    = {Ge, ZongYuan and McCool, Chris and Sanderson, Conrad and Wang, Peng and Liu, Lingqiao and Reid, Ian and Corke, Peter},
  title     = {VB100 Bird Dataset (video and audio)},
  year      = {2016},
  publisher = {Zenodo},
  doi       = {10.5281/zenodo.60375},
  url       = {https://doi.org/10.5281/zenodo.60375}
}
