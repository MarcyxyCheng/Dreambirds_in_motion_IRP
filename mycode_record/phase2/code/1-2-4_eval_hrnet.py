#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
04_eval_hrnet.py - æ¸…ç†ç‰ˆæœ¬
----------------
åœ¨ val.json ä¸Šè·‘æ¨ç† â†’ è®¡ç®— PCK@0.2
+ è¾“å‡ºå¯è§†åŒ–ç»“æœï¼ŒéªŒè¯æ¨¡å‹æ€§èƒ½
"""

import numpy as np
import cv2
import json
import random
import torch
import yaml
import datetime
import time
from pathlib import Path
from mmpose.apis import inference_topdown, init_model

# é…ç½®
# WORK_DIR = Path("workdir/hrnet17")
WORK_DIR = Path("workdir/hrnet17_simple")
# CFG = WORK_DIR / "bird17_hrnet_w32.py"
CFG = WORK_DIR / "bird17_hrnet_simple.py"
CKPT = WORK_DIR / "bird17_detector.pth"
VAL_JSON = Path("data/merged17/val.json")
SKELETON_YAML = Path("skeleton.yaml")
VIS_OUT = Path("vis_eval")
LOGBOOK_FILE = WORK_DIR / "evaluation_logbook.md"
VIS_OUT.mkdir(exist_ok=True)

N_SAMPLES = 500
VIS_RATIO = 0.1
PCK_THRESHOLD = 0.2

class EvaluationLogger:
    def __init__(self, logbook_path):
        self.logbook_path = Path(logbook_path)
        self.start_time = None
        self.eval_stats = {}
        self.logbook_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_logbook()
    
    def _init_logbook(self):
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        header = f"""# ğŸ” Bird 17-Keypoint HRNet Evaluation Logbook

**è¯„ä¼°å¼€å§‹æ—¶é—´**: {timestamp}
**é¡¹ç›®**: é¸Ÿç±»17å…³é”®ç‚¹æ£€æµ‹å™¨æ€§èƒ½è¯„ä¼°
**æ¨¡å‹**: HRNet-W32 (`bird17_detector.pth`)

---

## ğŸ“‹ è¯„ä¼°é…ç½®

| å‚æ•° | å€¼ |
|------|-----|
| è¯„ä¼°æ ·æœ¬æ•° | {N_SAMPLES} |
| PCKé˜ˆå€¼ | {PCK_THRESHOLD} |
| è®¾å¤‡ | {'CUDA' if torch.cuda.is_available() else 'CPU'} |

---

## ğŸš€ è¯„ä¼°è¿‡ç¨‹

"""
        self.logbook_path.write_text(header)
    
    def log_dataset_info(self, stats):
        stats_text = f"""
| ç»Ÿè®¡é¡¹ | æ•°å€¼ |
|--------|------|
| éªŒè¯å›¾åƒæ•° | {stats.get('val_images', 0)} |
| éªŒè¯æ ‡æ³¨æ•° | {stats.get('val_annotations', 0)} |
| å¹³å‡å…³é”®ç‚¹æ•° | {stats.get('avg_keypoints', 0):.1f} |

"""
        with open(self.logbook_path, 'a') as f:
            f.write(stats_text)
    
    def log_evaluation_start(self):
        self.start_time = time.time()
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        log_text = f"""
### {timestamp} - è¯„ä¼°å¼€å§‹
- âœ… æ¨¡å‹åŠ è½½æˆåŠŸ
- ğŸ” å¼€å§‹æ‰¹é‡æ¨ç†...

"""
        with open(self.logbook_path, 'a') as f:
            f.write(log_text)
    
    def log_evaluation_complete(self, results, success=True, error_msg=None):
        end_time = time.time()
        duration = end_time - self.start_time if self.start_time else 0
        duration_str = f"{duration//3600:.0f}h {(duration%3600)//60:.0f}m {duration%60:.0f}s"
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        
        if success and results:
            mean_pck = results.get('mean_pck', 0)
            if mean_pck > 0.7:
                performance = "ğŸŒŸ ä¼˜ç§€"
            elif mean_pck > 0.5:
                performance = "âœ… è‰¯å¥½"
            elif mean_pck > 0.3:
                performance = "âš ï¸ ä¸€èˆ¬"
            else:
                performance = "âŒ è¾ƒå·®"
            
            log_text = f"""
### {timestamp} - è¯„ä¼°å®Œæˆ âœ…

- â±ï¸ **æ€»è€—æ—¶**: {duration_str}
- ğŸ“Š **æ•´ä½“æ€§èƒ½**: {performance}
- ğŸ¯ **PCK@{PCK_THRESHOLD}**: {mean_pck:.4f}

## ğŸ“ˆ è¯¦ç»†ç»“æœ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| PCKå‡å€¼ | {mean_pck:.4f} |
| PCKæ ‡å‡†å·® | {results.get('std_pck', 0):.4f} |
| æœ€ä½³PCK | {results.get('max_pck', 0):.4f} |
| æœ€å·®PCK | {results.get('min_pck', 0):.4f} |
| æˆåŠŸç‡ | {results.get('success_rate', 0):.1%} |

**è¯„ä¼°å®Œæˆæ—¶é—´**: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        else:
            log_text = f"""
### {timestamp} - è¯„ä¼°å¤±è´¥ âŒ

- â±ï¸ **è€—æ—¶**: {duration_str}
- âŒ **é”™è¯¯**: {error_msg or "æœªçŸ¥é”™è¯¯"}

**å¤±è´¥æ—¶é—´**: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        with open(self.logbook_path, 'a') as f:
            f.write(log_text)

eval_logger = EvaluationLogger(LOGBOOK_FILE)

def check_files():
    missing_files = []
    if not CFG.exists():
        missing_files.append(f"é…ç½®æ–‡ä»¶: {CFG}")
    if not CKPT.exists():
        missing_files.append(f"æ¨¡å‹æ–‡ä»¶: {CKPT}")
    if not VAL_JSON.exists():
        missing_files.append(f"éªŒè¯æ•°æ®: {VAL_JSON}")
    
    if missing_files:
        print("âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶:")
        for file in missing_files:
            print(f"   - {file}")
        return False
    
    print("âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

def calculate_pck(gt_keypoints, pred_keypoints, img_width, img_height, threshold=0.2):
    dists = np.linalg.norm(gt_keypoints - pred_keypoints, axis=1)
    normalize_factor = max(img_width, img_height)
    normalized_dists = dists / normalize_factor
    pck = (normalized_dists < threshold).mean()
    return pck, normalized_dists

def draw_keypoints_and_skeleton(img, keypoints, skeleton_info):
    img_vis = img.copy()
    
    if skeleton_info:
        for connection in skeleton_info:
            pt1_idx, pt2_idx = connection
            if (keypoints[pt1_idx][0] > 0 and keypoints[pt1_idx][1] > 0 and 
                keypoints[pt2_idx][0] > 0 and keypoints[pt2_idx][1] > 0):
                
                pt1 = tuple(map(int, keypoints[pt1_idx]))
                pt2 = tuple(map(int, keypoints[pt2_idx]))
                cv2.line(img_vis, pt1, pt2, (0, 255, 0), 2)
    
    for i, (x, y) in enumerate(keypoints):
        if x > 0 and y > 0:
            cv2.circle(img_vis, (int(x), int(y)), 4, (0, 0, 255), -1)
    
    return img_vis

def main():
    print("ğŸ” å¼€å§‹è¯„ä¼°é¸Ÿç±»17å…³é”®ç‚¹æ£€æµ‹å™¨")
    print("=" * 60)
    
    # æ£€æŸ¥æ–‡ä»¶
    if not check_files():
        eval_logger.log_evaluation_complete({}, success=False, error_msg="å¿…è¦æ–‡ä»¶ç¼ºå¤±")
        return
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    try:
        model = init_model(
            str(CFG), 
            str(CKPT), 
            device='cuda:0' if torch.cuda.is_available() else 'cpu',
            cfg_options={"test_cfg": {"flip_test": False}}
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        eval_logger.log_evaluation_complete({}, success=False, error_msg=f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åŠ è½½éªŒè¯æ•°æ®
    print("ğŸ“Š åŠ è½½éªŒè¯æ•°æ®...")
    with open(VAL_JSON) as f:
        val_data = json.load(f)
    
    annotations = val_data["annotations"]
    images = {img["id"]: img for img in val_data["images"]}
    
    # æ•°æ®é›†ç»Ÿè®¡
    keypoint_counts = [ann['num_keypoints'] for ann in annotations]
    dataset_stats = {
        'val_images': len(images),
        'val_annotations': len(annotations),
        'avg_keypoints': np.mean(keypoint_counts)
    }
    
    eval_logger.log_dataset_info(dataset_stats)
    
    # åŠ è½½éª¨æ¶ä¿¡æ¯
    with open(SKELETON_YAML) as f:
        skeleton_data = yaml.safe_load(f)
    skeleton_connections = skeleton_data.get("skeleton", [])
    
    # å¼€å§‹è¯„ä¼°
    print(f"ğŸ¯ å¼€å§‹è¯„ä¼° (æŠ½æ · {N_SAMPLES} ä¸ª)")
    eval_logger.log_evaluation_start()
    
    if len(annotations) < N_SAMPLES:
        eval_annotations = annotations
    else:
        eval_annotations = random.sample(annotations, N_SAMPLES)
    
    pck_scores = []
    valid_predictions = 0
    vis_count = 0
    
    for i, ann in enumerate(eval_annotations):
        if i % 50 == 0:
            print(f"   è¿›åº¦: {i+1}/{len(eval_annotations)}")
        
        img_info = images[ann["image_id"]]
        img_path = Path("data") / img_info["file_name"]
        
        if not img_path.exists():
            continue
        
        try:
            # æ–°ç‰ˆæœ¬MMPoseçš„æ¨ç†æ–¹å¼
            results = inference_topdown(model, str(img_path))
            
            # ä¿®å¤ï¼šæ­£ç¡®è®¿é—®è¿”å›ç»“æœ
            if not results or len(results) == 0:
                continue
            
            # è·å–ç¬¬ä¸€ä¸ªç»“æœ
            result = results[0]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰pred_instances
            if not hasattr(result, 'pred_instances') or len(result.pred_instances.keypoints) == 0:
                continue
            
            pred_keypoints = result.pred_instances.keypoints[0]  # (17, 2)
            gt_keypoints = np.array(ann["keypoints"]).reshape(-1, 3)[:, :2]  # (17, 2)
            
            pck, _ = calculate_pck(
                gt_keypoints, pred_keypoints, 
                img_info["width"], img_info["height"], 
                PCK_THRESHOLD
            )
            
            pck_scores.append(pck)
            valid_predictions += 1
            
            # å¯è§†åŒ–
            if random.random() < VIS_RATIO and vis_count < 20:
                img = cv2.imread(str(img_path))
                if img is not None:
                    img_pred = draw_keypoints_and_skeleton(img, pred_keypoints, skeleton_connections)
                    
                    for x, y in gt_keypoints:
                        if x > 0 and y > 0:
                            cv2.circle(img_pred, (int(x), int(y)), 3, (0, 255, 0), 2)
                    
                    info_text = f"PCK: {pck:.3f}, ID: {ann['image_id']}"
                    cv2.putText(img_pred, info_text, (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    
                    output_file = VIS_OUT / f"eval_{ann['image_id']:06d}_pck{pck:.3f}.jpg"
                    cv2.imwrite(str(output_file), img_pred)
                    vis_count += 1
                    
        except Exception as e:
            print(f"âš ï¸  å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}")
            continue
    
    # è®¡ç®—æœ€ç»ˆç»“æœ
    if pck_scores:
        mean_pck = np.mean(pck_scores)
        std_pck = np.std(pck_scores)
        max_pck = max(pck_scores)
        min_pck = min(pck_scores)
        
        print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"   æœ‰æ•ˆé¢„æµ‹æ•°: {valid_predictions}/{len(eval_annotations)}")
        print(f"   PCK@{PCK_THRESHOLD}: {mean_pck:.4f} Â± {std_pck:.4f}")
        print(f"   æœ€ä½³PCK: {max_pck:.4f}")
        print(f"   æœ€å·®PCK: {min_pck:.4f}")
        
        # æ€§èƒ½è¯„ä¼°
        if mean_pck > 0.7:
            print("ğŸ‰ æ¨¡å‹æ€§èƒ½ä¼˜ç§€!")
        elif mean_pck > 0.5:
            print("âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½")
        elif mean_pck > 0.3:
            print("âš ï¸  æ¨¡å‹æ€§èƒ½ä¸€èˆ¬")
        else:
            print("âŒ æ¨¡å‹æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®é‡æ–°è®­ç»ƒ")
            
        eval_results = {
            'mean_pck': float(mean_pck),
            'std_pck': float(std_pck),
            'max_pck': float(max_pck),
            'min_pck': float(min_pck),
            'valid_predictions': valid_predictions,
            'total_samples': len(eval_annotations),
            'success_rate': valid_predictions / len(eval_annotations),
            'threshold': PCK_THRESHOLD
        }
        
        results_file = WORK_DIR / "eval_results.json"
        with open(results_file, 'w') as f:
            json.dump(eval_results, f, indent=2)
        
        eval_logger.log_evaluation_complete(eval_results, success=True)
        
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        eval_logger.log_evaluation_complete({}, success=False, error_msg="æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        return
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
    print(f"ğŸ“‹ è¯¦ç»†è¯„ä¼°æ—¥å¿—: {LOGBOOK_FILE}")

if __name__ == "__main__":
    main()