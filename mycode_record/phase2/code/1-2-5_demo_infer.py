
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
05_demo_infer.py img.jpg
------------------------
æ¨ç†å•å¼ å›¾ç‰‡ â†’ æ˜¾ç¤º/ä¿å­˜ result.jpg
âœ¨ é€‚é…æ–°çš„é…ç½®å’Œå¢å¼ºå¯è§†åŒ–
"""

import sys
import cv2
import numpy as np
import yaml
import torch
from pathlib import Path
from mmpose.apis import init_model, inference_topdown

# ================================================================
# è·¯å¾„é…ç½®
# ================================================================
WORK_DIR = Path("workdir/hrnet17_simple")
CFG = WORK_DIR / "bird17_hrnet_simple.py"
CKPT = WORK_DIR / "bird17_detector.pth"
SKELETON_YAML = Path("skeleton.yaml")

def load_skeleton_info():
    """åŠ è½½éª¨æ¶è¿æ¥ä¿¡æ¯"""
    try:
        with open(SKELETON_YAML) as f:
            skeleton_data = yaml.safe_load(f)
        return skeleton_data.get("skeleton", [])
    except:
        print("âš ï¸  æ— æ³•åŠ è½½éª¨æ¶ä¿¡æ¯ï¼Œå°†åªæ˜¾ç¤ºå…³é”®ç‚¹")
        return []

def draw_enhanced_pose(img, keypoints, skeleton_connections=None, confidence_threshold=0.3):
    """
    ç»˜åˆ¶å¢å¼ºçš„å§¿æ€å¯è§†åŒ–
    Args:
        img: è¾“å…¥å›¾åƒ
        keypoints: å…³é”®ç‚¹åæ ‡ (17, 2) æˆ– (17, 3)
        skeleton_connections: éª¨æ¶è¿æ¥ä¿¡æ¯
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
    """
    img_vis = img.copy()
    h, w = img.shape[:2]
    
    # å¦‚æœå…³é”®ç‚¹æœ‰ç½®ä¿¡åº¦ä¿¡æ¯
    if keypoints.shape[1] == 3:
        kpts_xy = keypoints[:, :2]
        kpts_conf = keypoints[:, 2]
    else:
        kpts_xy = keypoints
        kpts_conf = np.ones(len(keypoints))  # å‡è®¾éƒ½å¯è§
    
    # ç»˜åˆ¶éª¨æ¶è¿æ¥
    if skeleton_connections:
        for connection in skeleton_connections:
            pt1_idx, pt2_idx = connection
            
            # æ£€æŸ¥å…³é”®ç‚¹æ˜¯å¦æœ‰æ•ˆä¸”ç½®ä¿¡åº¦è¶³å¤Ÿ
            if (kpts_conf[pt1_idx] > confidence_threshold and 
                kpts_conf[pt2_idx] > confidence_threshold and
                kpts_xy[pt1_idx][0] > 0 and kpts_xy[pt1_idx][1] > 0 and
                kpts_xy[pt2_idx][0] > 0 and kpts_xy[pt2_idx][1] > 0):
                
                pt1 = tuple(map(int, kpts_xy[pt1_idx]))
                pt2 = tuple(map(int, kpts_xy[pt2_idx]))
                
                # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´çº¿æ¡ç²—ç»†å’Œé¢œè‰²
                avg_conf = (kpts_conf[pt1_idx] + kpts_conf[pt2_idx]) / 2
                thickness = max(1, int(3 * avg_conf))
                color_intensity = int(255 * avg_conf)
                
                cv2.line(img_vis, pt1, pt2, (0, color_intensity, 0), thickness)
    
    # ç»˜åˆ¶å…³é”®ç‚¹
    for i, ((x, y), conf) in enumerate(zip(kpts_xy, kpts_conf)):
        if conf > confidence_threshold and x > 0 and y > 0:
            # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´ç‚¹çš„å¤§å°å’Œé¢œè‰²
            radius = max(2, int(6 * conf))
            color_intensity = int(255 * conf)
            
            # ç»˜åˆ¶å…³é”®ç‚¹
            cv2.circle(img_vis, (int(x), int(y)), radius, (0, 0, color_intensity), -1)
            cv2.circle(img_vis, (int(x), int(y)), radius+1, (255, 255, 255), 1)
            
            # å¯é€‰ï¼šæ˜¾ç¤ºå…³é”®ç‚¹ç¼–å·
            if conf > 0.5:  # åªåœ¨é«˜ç½®ä¿¡åº¦æ—¶æ˜¾ç¤ºç¼–å·
                cv2.putText(img_vis, str(i), (int(x)+8, int(y)-8), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    
    return img_vis

def add_info_overlay(img, keypoints, skeleton_connections):
    """æ·»åŠ ä¿¡æ¯è¦†ç›–å±‚"""
    img_info = img.copy()
    h, w = img.shape[:2]
    
    # åˆ›å»ºåŠé€æ˜è¦†ç›–å±‚
    overlay = np.zeros((h, w, 3), dtype=np.uint8)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if keypoints.shape[1] == 3:
        visible_count = np.sum(keypoints[:, 2] > 0.3)
        avg_confidence = np.mean(keypoints[keypoints[:, 2] > 0, 2])
    else:
        visible_count = np.sum((keypoints[:, 0] > 0) & (keypoints[:, 1] > 0))
        avg_confidence = 1.0
    
    # æ·»åŠ æ–‡æœ¬ä¿¡æ¯
    info_texts = [
        f"Detected Keypoints: {visible_count}/17",
        f"Avg Confidence: {avg_confidence:.3f}",
        f"Model: Bird17 HRNet",
        f"Framework: MMPose"
    ]
    
    # ç»˜åˆ¶ä¿¡æ¯æ¡†
    text_area_height = len(info_texts) * 25 + 20
    cv2.rectangle(overlay, (10, 10), (300, text_area_height), (50, 50, 50), -1)
    
    for i, text in enumerate(info_texts):
        y_pos = 30 + i * 25
        cv2.putText(overlay, text, (20, y_pos), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
    
    # æ··åˆè¦†ç›–å±‚
    alpha = 0.7
    img_info = cv2.addWeighted(img_info, 1-alpha, overlay, alpha, 0)
    
    return img_info

def main():
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python 05_demo_infer.py <image_path>")
        print("ç¤ºä¾‹: python 05_demo_infer.py bird.jpg")
        return
    
    img_path = Path(sys.argv[1])
    
    if not img_path.exists():
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
        return
    
    print(f"ğŸ¦œ å¼€å§‹æ¨ç†é¸Ÿç±»å§¿æ€: {img_path}")
    print("=" * 50)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not CFG.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CFG}")
        return
    
    if not CKPT.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {CKPT}")
        return
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    try:
        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        model = init_model(str(CFG), str(CKPT), device=device)
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {device})")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åŠ è½½éª¨æ¶ä¿¡æ¯
    skeleton_connections = load_skeleton_info()
    
    # æ¨ç†
    print("ğŸ” æ‰§è¡Œå§¿æ€æ£€æµ‹...")
    try:
        # æ–°ç‰ˆæœ¬MMPoseçš„æ¨ç†æ–¹å¼
        results = inference_topdown(model, str(img_path))
        
        # ä¿®å¤ï¼šæ­£ç¡®è®¿é—®è¿”å›ç»“æœ
        if not results or len(results) == 0:
            print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„å§¿æ€")
            return
        
        # è·å–ç¬¬ä¸€ä¸ªç»“æœ
        result = results[0]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰pred_instances
        if not hasattr(result, 'pred_instances') or len(result.pred_instances.keypoints) == 0:
            print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„å§¿æ€")
            return
            
        keypoints = result.pred_instances.keypoints[0]
        print(f"âœ… æ£€æµ‹æˆåŠŸï¼Œæ‰¾åˆ° {len(keypoints)} ä¸ªå…³é”®ç‚¹")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return
    
    # è¯»å–åŸå›¾
    img = cv2.imread(str(img_path))
    if img is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {img_path}")
        return
    
    # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
    print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    
    # åŸºç¡€å§¿æ€å¯è§†åŒ–
    img_pose = draw_enhanced_pose(img, keypoints, skeleton_connections)
    
    # å¸¦ä¿¡æ¯è¦†ç›–çš„ç‰ˆæœ¬
    img_info = add_info_overlay(img_pose, keypoints, skeleton_connections)
    
    # ä¿å­˜ç»“æœ
    output_files = []
    
    # ä¿å­˜åŸºç¡€ç‰ˆæœ¬
    basic_output = img_path.with_suffix(f".pose{img_path.suffix}")
    cv2.imwrite(str(basic_output), img_pose)
    output_files.append(basic_output)
    
    # ä¿å­˜ä¿¡æ¯ç‰ˆæœ¬
    info_output = img_path.with_suffix(f".pose_info{img_path.suffix}")
    cv2.imwrite(str(info_output), img_info)
    output_files.append(info_output)
    
    # å¹¶æ’å¯¹æ¯”ç‰ˆæœ¬
    comparison = np.hstack([img, img_pose])
    comp_output = img_path.with_suffix(f".comparison{img_path.suffix}")
    cv2.imwrite(str(comp_output), comparison)
    output_files.append(comp_output)
    
    print("âœ… å§¿æ€æ£€æµ‹å®Œæˆ!")
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    for output_file in output_files:
        print(f"   - {output_file}")
    
    # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
    if keypoints.shape[1] == 3:
        visible_kpts = np.sum(keypoints[:, 2] > 0.3)
        avg_conf = np.mean(keypoints[keypoints[:, 2] > 0, 2])
        print(f"\nğŸ“Š æ£€æµ‹ç»Ÿè®¡:")
        print(f"   å¯è§å…³é”®ç‚¹: {visible_kpts}/17")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.3f}")
    
    print(f"\nğŸ¯ ç°åœ¨å¯ä»¥ç»§ç»­Pipelineæ­¥éª¤1.3!")

if __name__ == "__main__":
    main()